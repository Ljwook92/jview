{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language. It combines linguistics, computer science, and machine learning to process text and speech in ways that are meaningful and useful. Common applications of NLP include machine translation, sentiment analysis, chatbots, and speech recognition. By bridging the gap between human communication and computer understanding, NLP plays a key role in modern technology, from virtual assistants to search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint            # For pretty-printing data structures\n",
    "from bs4 import BeautifulSoup        # Import BeautifulSoup for HTML parsing\n",
    "from urllib.request import urlopen   # For opening URLs\n",
    "\n",
    "import requests                     # For adding header\n",
    "import pandas as pd\n",
    "import re                           # For numeric columns\n",
    "import numpy as np                  # For numeric columns\n",
    "\n",
    "import matplotlib.pyplot as plt     # For visualization\n",
    "\n",
    "\n",
    "import nltk                             # For Stopwords\n",
    "import string                           # For Stopwords\n",
    "from nltk.corpus import stopwords       # For Stopwords\n",
    "from nltk.tokenize import word_tokenize # For Stopwords\n",
    "from string import punctuation          # For Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Choose a Website and Identify Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I chose the Wikipedia website for this task. As part of my ongoing research on wildfires in the USA, I was looking for a comprehensive list of wildfire events. This website provides a well-structured table that summarizes wildfire history across the country. The table includes clearly organized information such as the year, size, the name of the wildfire, and area.  \n",
    "I select this table not only because it is relevant to my research topic, but also because the HTML structure of the table is relatively simple and consistent, making it easier to scrape.  \n",
    "\n",
    "(Ref: https://en.wikipedia.org/wiki/List_of_wildfires)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scrape Data Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Load website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target website\n",
    "myurl = 'https://en.wikipedia.org/wiki/List_of_wildfires'\n",
    "\n",
    "# Add header for avoiding error: 'HTTP Error 403: Forbidden'\n",
    "# ref: https://stackoverflow.com/questions/77129954/error-403-webscraping-project-using-beautifulsoup\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/117.0'}\n",
    "\n",
    "# Request with header \n",
    "request_id = requests.get(myurl, \n",
    "                          headers = headers)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup with the chosen parser\n",
    "soupified = BeautifulSoup(request_id.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Find all tables in the website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract only table including `Year`, `Size`, `Name`, and `Area`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 0: 1 rows × 2 columns\n",
      "Table 1: 132 rows × 5 columns\n",
      "Table 2: 16 rows × 1 columns\n",
      "Table 3: 5 rows × 2 columns\n"
     ]
    }
   ],
   "source": [
    "def extract_table_data(soup_object):\n",
    "    tables_data = []\n",
    "    \n",
    "    for table_index, table in enumerate(soup_object.find_all('table')):\n",
    "        table_data = []\n",
    "        rows = table.find_all('tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            # Handle both header (th) and data (td) cells\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            row_data = [cell.get_text(strip=True) for cell in cells]\n",
    "            if row_data:  # Only add non-empty rows\n",
    "                table_data.append(row_data)\n",
    "        \n",
    "        if table_data:  # Only add non-empty tables\n",
    "            tables_data.append({\n",
    "                'table_index': table_index,\n",
    "                'data': table_data,\n",
    "                'rows': len(table_data),\n",
    "                'columns': len(table_data[0]) if table_data else 0\n",
    "            })\n",
    "    \n",
    "    return tables_data\n",
    "\n",
    "table_info = extract_table_data(soupified)\n",
    "for table in table_info:\n",
    "    print(f\"Table {table['table_index']}: {table['rows']} rows × {table['columns']} columns\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
